{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==========================================\n# 1. SETUP E DOWNLOAD DIRETTO (BYPASS DEFENDER)\n# ==========================================\n%pip install -q transformers datasets emoji accelerate > /dev/null 2>&1\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nimport torch\nimport emoji\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import fetch_20newsgroups\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\nfrom datasets import Dataset, load_dataset\n\n# Verifica GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"üñ•Ô∏è Stai utilizzando: {device.upper()}\")\n\n# --- DOWNLOAD DATI DA GITHUB A KAGGLE ---\n# Scarichiamo i raw file direttamente nella cartella di lavoro di Kaggle (/kaggle/working/)\nprint(\"‚¨áÔ∏è Scaricando APTNER direttamente da GitHub...\")\n!wget -q https://raw.githubusercontent.com/wangxuren/APTNER/master/APTNERtrain.txt -O /kaggle/working/train.txt\n!wget -q https://raw.githubusercontent.com/wangxuren/APTNER/master/APTNERtest.txt -O /kaggle/working/test.txt\n\n# Definiamo i percorsi dei file appena scaricati\nFILE_TRAIN = \"/kaggle/working/train.txt\"\nFILE_TEST = \"/kaggle/working/test.txt\"\nMODELLO_BASE = \"ehsanaghaei/SecureBERT\"\n\nprint(\"‚¨áÔ∏è Scaricamento APTNER completato!\")\n\n# --- FUNZIONI (Clean & Parser) ---\ndef clean_and_mask(text):\n    if not isinstance(text, str): return \"\"\n    \n    # 1. Rimuovi Emoji\n    text = emoji.replace_emoji(text, replace='')\n    # --- NUOVA REGOLA: MASCHERA LE CVE ---\n    # Trasforma \"CVE-2023-4455\" o \"CVE-2021-100\" in \"[CVE]\"\n    # Il modello imparer√† che il token [CVE] √® SEMPRE una cosa brutta.\n    text = re.sub(r'CVE-\\d{4}-\\d+', '[CVE]', text, flags=re.IGNORECASE)\n    \n    # 2. Maschera Link Telegram\n    text = re.sub(r'(?:https?://)?(?:www\\.)?(?:t\\.me|telegram\\.me)/[a-zA-Z0-9_]+', '[TG_LINK]', text)\n    \n    # 3. Maschera URL standard\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n    \n    # 4. Maschera IP Address (IPv4)\n    text = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', '[IP]', text)\n    \n    # 5. Maschera Domini generici\n    text = re.sub(r'\\b(?:[a-zA-Z0-9-]+\\.)+(?:com|org|net|io|ru|cn|it|uk|gov)\\b', '[DOMAIN]', text)\n    \n    # 6. Pulizia spazi\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndef processa_aptner_multiplo(lista_files):\n    all_sentences = []\n    all_labels = []\n    for input_file in lista_files:\n        print(f\"üîÑ Processando {os.path.basename(input_file)}...\")\n        if not os.path.exists(input_file): continue\n        \n        current_words = []\n        has_entity = False \n        with open(input_file, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n            lines = f.readlines()\n\n        for line in lines:\n            line = line.strip()\n            if not line:\n                if current_words:\n                    full_sentence = \" \".join(current_words)\n                    label = 1 if has_entity else 0\n                    all_sentences.append(full_sentence)\n                    all_labels.append(label)\n                    current_words = []\n                    has_entity = False\n                continue\n            parts = line.split()\n            if len(parts) < 2: continue\n            if parts[-1] != 'O': has_entity = True\n            current_words.append(parts[0])\n    return pd.DataFrame({'text': all_sentences, 'label': all_labels})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T15:04:24.517772Z","iopub.execute_input":"2025-12-08T15:04:24.517979Z","iopub.status.idle":"2025-12-08T15:06:29.419662Z","shell.execute_reply.started":"2025-12-08T15:04:24.517961Z","shell.execute_reply":"2025-12-08T15:06:29.418855Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"},{"name":"stderr","text":"2025-12-08 15:06:07.333610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765206367.543771      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765206367.601741      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"üñ•Ô∏è Stai utilizzando: CUDA\n‚¨áÔ∏è Scaricando APTNER direttamente da GitHub...\n‚¨áÔ∏è Scaricamento APTNER completato!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==========================================\n# 2. CARICAMENTO DATI (AGGIORNATO CON CHAT REALI)\n# ==========================================\n\n# A. Caricamento APTNER (dai file scaricati col codice sopra)\nprint(\"üìÇ Caricamento file locali...\")\n# Assicurati che FILE_TRAIN e FILE_TEST siano definiti nella cella precedente\n# Se d√† errore qui, controlla di aver eseguito la Cella 1\ndf_cyber = processa_aptner_multiplo([FILE_TRAIN, FILE_TEST])\n\n# FILTRO: Solo frasi con entit√† (Label 1)\ndf_cyber = df_cyber[df_cyber['label'] == 1]\nprint(f\"‚úÖ APTNER (Cyber Only): {len(df_cyber)} frasi.\")\n\n# ==========================================\n# DATA AUGMENTATION: INIEZIONE CVE\n# ==========================================\nprint(\"üíâ Iniettando esempi di Vulnerabilit√† (CVE) per migliorare il modello...\")\n\ncve_data = [\n    \"Critical vulnerability CVE-2023-2251 found in Windows Server.\",\n    \"Please patch immediately against CVE-2024-1001 to prevent remote code execution.\",\n    \"Zero-day exploit detected targeting CVE-2021-44228 (Log4Shell).\",\n    \"New proof of concept available for CVE-2023-0001 on GitHub.\",\n    \"Hackers are scanning for unpatched systems with CVE-2022-9999.\",\n    \"The new update fixes CVE-2023-5555 and strictly sanitizes input.\",\n    \"Warning: Ransomware groups are exploiting CVE-2024-3000.\",\n    \"Buffer overflow vulnerability identified as CVE-2020-1234.\",\n    \"Check if your firewall blocks attempts related to CVE-2025-0001.\",\n    \"High severity flaw CVE-2023-9876 allows unauthorized access.\"\n]\n\n# Creiamo un mini-dataframe con Label 1 (Cyber)\ndf_cve = pd.DataFrame({'text': cve_data, 'label': [1] * len(cve_data)})\n\n# Lo aggiungiamo al dataset Cyber originale\ndf_cyber = pd.concat([df_cyber, df_cve], ignore_index=True)\n\nprint(f\"‚úÖ Aggiunti {len(df_cve)} esempi di CVE. Totale Cyber: {len(df_cyber)}\")\n\n# ==========================================\n# B. CARICAMENTO RUMORE (POTENZIATO: SMS + DAILY DIALOG)\n# ==========================================\nprint(\"‚¨áÔ∏è Scaricando dataset Rumore Misto (SMS + Dialoghi)...\")\n\n# 1. Dataset SMS SPAM (Chat brevi, slang) - Ne prendiamo TUTTI quelli disponibili\ndataset_sms = load_dataset(\"sms_spam\", split=\"train\")\ndf_sms = pd.DataFrame(dataset_sms)\ndf_sms = df_sms.rename(columns={'sms': 'text'})\ndf_sms['label'] = 0\n\n# --- 2. DATASET GO_EMOTIONS (Reddit - Per discussioni e chat) ---\nprint(\"   2. Caricamento Reddit (go_emotions)...\")\n# Questo √® nativo e sicuro, non dar√† errori di script\ndataset_reddit = load_dataset(\"go_emotions\", split=\"train\")\ndf_reddit = pd.DataFrame(dataset_reddit)\ndf_reddit = df_reddit[['text']] # Teniamo solo il testo\ndf_reddit['label'] = 0\n\n# --- 3. UNIONE E PULIZIA ---\nprint(\"   3. Fusione dataset...\")\n# Uniamo SMS e Reddit\ndf_noise = pd.concat([df_sms, df_reddit], ignore_index=True)\n\n# Pulizia: Rimuoviamo messaggi troppo corti o tag di rimozione reddit\ndf_noise = df_noise[df_noise['text'].str.len() > 4]\ndf_noise = df_noise[df_noise['text'] != '[deleted]']\ndf_noise = df_noise[df_noise['text'] != '[removed]']\n\n# --- 4. BILANCIAMENTO ---\n# Vogliamo che il rumore sia pari ai dati Cyber (circa 7000)\ntarget_size = len(df_cyber)\n\nif len(df_noise) > target_size:\n    # Abbiamo tantissimi dati (5k SMS + 58k Reddit), quindi campioniamo\n    df_noise = df_noise.sample(n=target_size, random_state=42)\n    print(f\"‚úÖ Bilanciamento perfetto: estratti {target_size} messaggi di rumore su {len(df_sms)+len(df_reddit)} totali.\")\nelse:\n    print(f\"‚ö†Ô∏è Nota: Usiamo tutto il rumore disponibile ({len(df_noise)} frasi).\")\n\nprint(f\"‚úÖ Dataset Rumore Pronto.\")\nprint(f\"   Esempio (SMS): {df_sms.iloc[0]['text'] if not df_sms.empty else 'N/A'}\")\nprint(f\"   Esempio (Reddit): {df_reddit.iloc[0]['text']}\")\n\n# ==========================================\n# C. UNIONE FINALE E PREPROCESSING\n# ==========================================\nprint(\"üßπ Unione con Cyber e Masking...\")\ndf_full = pd.concat([df_cyber, df_noise]).sample(frac=1).reset_index(drop=True)\n\n# Applichiamo la pulizia\ndf_full['text_cleaned'] = df_full['text'].apply(clean_and_mask)\n\n# Salvataggio CSV per controllo\ndf_full.to_csv(\"dataset_training_final.csv\", index=False)\nprint(\"üíæ Dataset salvato per verifica in 'dataset_training_final.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T15:06:29.421959Z","iopub.execute_input":"2025-12-08T15:06:29.423113Z","iopub.status.idle":"2025-12-08T15:06:42.130877Z","shell.execute_reply.started":"2025-12-08T15:06:29.423088Z","shell.execute_reply":"2025-12-08T15:06:42.130243Z"}},"outputs":[{"name":"stdout","text":"üìÇ Caricamento file locali...\nüîÑ Processando train.txt...\nüîÑ Processando test.txt...\n‚úÖ APTNER (Cyber Only): 7006 frasi.\nüíâ Iniettando esempi di Vulnerabilit√† (CVE) per migliorare il modello...\n‚úÖ Aggiunti 10 esempi di CVE. Totale Cyber: 7016\n‚¨áÔ∏è Scaricando dataset Rumore Misto (SMS + Dialoghi)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"641f4e488af04a05b7e4987627957bac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/359k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"193e765084444115bc00024aa0134ac8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5574 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6369f82dba84d9b8c2f9ec8dd3caf5d"}},"metadata":{}},{"name":"stdout","text":"   2. Caricamento Reddit (go_emotions)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8efb00f9a644fb8ac96d55037567c95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"simplified/train-00000-of-00001.parquet:   0%|          | 0.00/2.77M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59679c3ce06e4f09b60b5286113eff87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"simplified/validation-00000-of-00001.par(‚Ä¶):   0%|          | 0.00/350k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c9cda834430436baca7d12048be75c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"simplified/test-00000-of-00001.parquet:   0%|          | 0.00/347k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5b95e90924d481aa189468297f56670"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/43410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce138eb5c2c74252b64d9349128417b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5426 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2f64abcf3e64f879bb0d19e453b1266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5427 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bcefd1b503a4423b2c1b2ef8e1b24ab"}},"metadata":{}},{"name":"stdout","text":"   3. Fusione dataset...\n‚úÖ Bilanciamento perfetto: estratti 7016 messaggi di rumore su 48984 totali.\n‚úÖ Dataset Rumore Pronto.\n   Esempio (SMS): Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n\n   Esempio (Reddit): My favourite food is anything I didn't have to cook myself.\nüßπ Unione con Cyber e Masking...\nüíæ Dataset salvato per verifica in 'dataset_training_final.csv'\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==========================================\n# 2.1 PREPARAZIONE DATASET PER HUGGINGFACE\n# ==========================================\n\ntrain_df, eval_df = train_test_split(df_full, test_size=0.2, random_state=42, stratify=df_full['label'])\n\n# Convertiamo da Pandas a HuggingFace Dataset\ntrain_dataset_raw = Dataset.from_pandas(train_df)\neval_dataset_raw = Dataset.from_pandas(eval_df)\n\n# Inizializziamo il tokenizer (serve caricarlo ORA per mappare i dati)\ntokenizer = RobertaTokenizer.from_pretrained(MODELLO_BASE)\n\ndef tokenize_function(examples):\n    # Tokenizziamo la colonna 'text_cleaned' (quella mascherata)\n    return tokenizer(examples[\"text_cleaned\"], padding=\"max_length\", truncation=True, max_length=128)\n\nprint(\"‚öôÔ∏è Tokenizzazione in corso...\")\n# Applichiamo la tokenizzazione\ntokenized_train = train_dataset_raw.map(tokenize_function, batched=True)\ntokenized_eval = eval_dataset_raw.map(tokenize_function, batched=True)\n\n# Rinominiamo la colonna label se necessario e rimuoviamo colonne inutili\n# DistilBERT si aspetta le colonne: 'input_ids', 'attention_mask', 'labels'\ntokenized_train = tokenized_train.remove_columns(['text', 'text_cleaned', '__index_level_0__'])\ntokenized_eval = tokenized_eval.remove_columns(['text', 'text_cleaned', '__index_level_0__'])\n\n# Impostiamo il formato per PyTorch\ntokenized_train.set_format(\"torch\")\ntokenized_eval.set_format(\"torch\")\n\nprint(\"‚úÖ Dataset pronti per il training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T15:06:42.131566Z","iopub.execute_input":"2025-12-08T15:06:42.131813Z","iopub.status.idle":"2025-12-08T15:06:49.141433Z","shell.execute_reply.started":"2025-12-08T15:06:42.131795Z","shell.execute_reply":"2025-12-08T15:06:49.140858Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ace583891524886acc76fa10649b708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"713be16286704525b52a0f96fefd1d17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fe9b702edd24b80894d431b77d3c780"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dbb60517f604079a7f73cff3ff3bc9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"565538f5ffe14452822515bb52bd8e8e"}},"metadata":{}},{"name":"stdout","text":"‚öôÔ∏è Tokenizzazione in corso...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11225 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"154805a2ea0c4e4bbce5e94015c8eb5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b794663ec9ed4fe69aaf74ca2061b7ee"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Dataset pronti per il training!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==========================================\n# 3. FINE-TUNING E PREDIZIONE\n# ==========================================\n\n# Setup Modello\nmodel = RobertaForSequenceClassification.from_pretrained(MODELLO_BASE, num_labels=2).to(device)\n\n# Argomenti Training\ntraining_args = TrainingArguments(\n    output_dir=\"./distilbert_cti_output\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16, # Se usi GPU P100 puoi provare 32\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,             # 3 epoche per sicurezza\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    logging_steps=50,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train, \n    eval_dataset=tokenized_eval,   \n    tokenizer=tokenizer,           \n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n)\n\nprint(\"üî• Inizio Addestramento...\")\ntrainer.train()\n\n# Salvataggio Modello per uso futuro (nella cartella Output di Kaggle)\nmodel.save_pretrained(\"./modello_finale\")\ntokenizer.save_pretrained(\"./modello_finale\")\nprint(\"‚úÖ Modello salvato in ./modello_finale\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T15:06:49.142199Z","iopub.execute_input":"2025-12-08T15:06:49.142461Z","iopub.status.idle":"2025-12-08T15:16:22.453154Z","shell.execute_reply.started":"2025-12-08T15:06:49.142434Z","shell.execute_reply":"2025-12-08T15:16:22.452472Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3229719adac24c429629c89ba7485e23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0b6fc4fd6849da869889bfcb0b78b3"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ehsanaghaei/SecureBERT and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_47/2696107151.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"üî• Inizio Addestramento...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1053' max='1053' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1053/1053 09:25, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000400</td>\n      <td>0.006155</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000100</td>\n      <td>0.002628</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000100</td>\n      <td>0.000567</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Modello salvato in ./modello_finale\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import shutil\nimport os\nfrom IPython.display import FileLink\n\n# ==========================================\n# CONFIGURAZIONE\n# ==========================================\n# Il nome della cartella che vuoi scaricare (quella creata dal tuo training)\nCARTELLA_DA_SCARICARE = \"./modello_finale\" \n\n# Il nome che vuoi dare al file zip finale\nNOME_FILE_ZIP = \"modello_finale\"\n\n# ==========================================\n# COMPRESSIONE E DOWNLOAD\n# ==========================================\n# Verifica se la cartella esiste\nif os.path.exists(CARTELLA_DA_SCARICARE):\n    print(f\"üì¶ Sto comprimendo la cartella '{CARTELLA_DA_SCARICARE}'...\")\n    \n    # Crea l'archivio (shutil aggiunge .zip in automatico)\n    shutil.make_archive(NOME_FILE_ZIP, 'zip', CARTELLA_DA_SCARICARE)\n    \n    print(f\"‚úÖ Compressione riuscita! File creato: {NOME_FILE_ZIP}.zip\")\n    print(\"‚¨áÔ∏è Clicca sul link qui sotto per scaricare il tuo modello:\")\n    \n    # Genera il link cliccabile\n    display(FileLink(f'{NOME_FILE_ZIP}.zip'))\nelse:\n    print(f\"‚ùå ERRORE: La cartella '{CARTELLA_DA_SCARICARE}' non esiste.\")\n    print(\"Controlla di aver eseguito la cella di salvataggio (model.save_pretrained).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T15:16:22.454178Z","iopub.execute_input":"2025-12-08T15:16:22.454463Z","iopub.status.idle":"2025-12-08T15:16:46.643028Z","shell.execute_reply.started":"2025-12-08T15:16:22.454442Z","shell.execute_reply":"2025-12-08T15:16:46.642203Z"}},"outputs":[{"name":"stdout","text":"üì¶ Sto comprimendo la cartella './modello_finale'...\n‚úÖ Compressione riuscita! File creato: modello_finale.zip\n‚¨áÔ∏è Clicca sul link qui sotto per scaricare il tuo modello:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/modello_finale.zip","text/html":"<a href='modello_finale.zip' target='_blank'>modello_finale.zip</a><br>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import os\nimport re\nimport torch\nimport emoji\nimport pandas as pd\nfrom pymongo import MongoClient\nfrom tqdm import tqdm\n\n# ==============================================================================\n# 1. CONFIGURAZIONE\n# ==============================================================================\nMONGO_URI = \"mongodb://DPA_Project_ReadOnly:DPA_sd_2025@130.192.238.49:27015/?authSource=admin\"\nDB_NAME = \"GroupMonitoringRelease\"\n# Assicurati di puntare al modello DISTILBERT che abbiamo addestrato\nPERCORSO_MODELLO = \"./modello_finale\" \nSOGLIA_CYBER = 0.60  # Soglia alta: Vogliamo solo messaggi sicuri per il report\nOUTPUT_FOLDER = \"Results_CTI_Enriched\"\n\nif not os.path.exists(OUTPUT_FOLDER):\n    os.makedirs(OUTPUT_FOLDER)\n\n# Gruppi Target\nGRUPPI_DA_ANALIZZARE = [\n    \"Cyber Security - Information Security - IT Security - Experts\",\n    \"Hacking Realm\",\n    \"VirusCheck Chat\",\n    \"DDOS‰∫§ÊµÅÊîªÂáª\",\n    \"HTTP Injector\",\n    \"HTTP Injector Chat\",\n    \"Mikrotik-Training\",\n    \"Only Dark\",\n    \"–¢–µ–Ω–µ–≤–æ–π –î–∞—Ä–∫–Ω–µ—Ç –ß–∞—Ç\",\n    \"–•–∞–∫–µ—Ä—ã |–ß–∞—Ç| ùìóùì™ùì¨ùì¥ùìÆùìªùìº ùì¨ùì±ùì™ùìΩ\",\n    \"–ß–∞—Ç –¥–æ–∫—Å–µ—Ä–æ–≤\",\n    \"HackDroids ‚Äî –ß–∞—Ç\"\n]\n\nLIMITE_MESSAGGI = None # Metti un numero (es. 5000) per test veloci\n\n# ==============================================================================\n# 2. FUNZIONI UTILI\n# ==============================================================================\ndef get_user_id(doc):\n    \"\"\"Estrae l'ID utente pulito dall'oggetto Telegram complessa\"\"\"\n    try:\n        from_id = doc.get(\"from_id\")\n        if isinstance(from_id, dict):\n            # Gestisce sia PeerUser che PeerChannel\n            return from_id.get(\"user_id\") or from_id.get(\"channel_id\")\n        return from_id # Fallback se √® gi√† int (raro)\n    except:\n        return None\n\ndef clean_and_mask(text):\n    if not isinstance(text, str): return \"\"\n    text = emoji.replace_emoji(text, replace='')\n    # Le stesse regole usate nel training\n    text = re.sub(r'CVE-\\d{4}-\\d+', '[CVE]', text, flags=re.IGNORECASE)\n    text = re.sub(r'(?:https?://)?(?:www\\.)?(?:t\\.me|telegram\\.me)/[a-zA-Z0-9_]+', '[TG_LINK]', text)\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n    text = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', '[IP]', text)\n    text = re.sub(r'\\b(?:[a-zA-Z0-9-]+\\.)+(?:com|org|net|io|ru|cn|it|uk|gov)\\b', '[DOMAIN]', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef classifica_batch(testi, tokenizer, model, device):\n    \"\"\"Inferenza su batch di testi\"\"\"\n    testi_puliti = [clean_and_mask(t) for t in testi]\n    try:\n        inputs = tokenizer(testi_puliti, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        return probs[:, 1].tolist()\n    except Exception:\n        return [0.0] * len(testi)\n\n# ==============================================================================\n# 3. SETUP\n# ==============================================================================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚öôÔ∏è  Device: {device.upper()}\")\n\nprint(\"üîå Connessione DB...\")\nclient = MongoClient(MONGO_URI)\ndb = client[DB_NAME]\n\n# --- A. COSTRUZIONE MAPPA METADATI ---\n# Scarichiamo tutti i gruppi una volta sola per avere i metadati pronti\nprint(\"üìñ Costruzione Registro Gruppi...\")\ngroup_meta_map = {}\nfor g in db[\"groups\"].find({\"chat_name\": {\"$in\": GRUPPI_DA_ANALIZZARE}}):\n    coll_name = g.get(\"collection_name\")\n    if coll_name:\n        group_meta_map[coll_name] = {\n            \"group_tele_id\": g.get(\"id\"),     # ID Telegram del gruppo\n            \"group_name\": g.get(\"chat_name\"),\n            \"topic\": g.get(\"topic\", \"N/A\"),\n            \"country\": g.get(\"country\", \"N/A\"),\n            \"language\": g.get(\"language\", \"N/A\")\n        }\nprint(f\"‚úÖ Mappati {len(group_meta_map)} gruppi target.\")\n\n# --- B. CARICAMENTO MODELLO ---\ntry:\n    # Nota: Assicurati di usare DistilBertTokenizer se hai trainato DistilBert\n    # SecureBERT usa l'architettura RoBERTa\n    tokenizer = RobertaTokenizer.from_pretrained(PERCORSO_MODELLO)\n    model = RobertaForSequenceClassification.from_pretrained(PERCORSO_MODELLO).to(device)\n    model.eval()\n    print(\"‚úÖ Modello caricato.\")\nexcept Exception as e:\n    print(f\"‚ùå Errore modello: {e}\")\n    exit()\n\n# ==============================================================================\n# 4. LOOP DI ANALISI\n# ==============================================================================\nBATCH_SIZE = 64\n\nfor coll_name, meta in group_meta_map.items():\n    print(f\"\\nüìÇ Analisi: {meta['group_name']} ({meta['topic']})\")\n    \n    # Carichiamo i messaggi ordinati per DATA CRESCENTE (vecchi -> nuovi)\n    # Questo rende facilissimo prendere il contesto (i-1 √® il precedente)\n    cursor = db[coll_name].find({}).sort(\"date\", 1) \n    if LIMITE_MESSAGGI: cursor = cursor.limit(LIMITE_MESSAGGI)\n    \n    msgs = list(cursor)\n    total_msgs = len(msgs)\n    if total_msgs == 0: continue\n    \n    risultati = []\n    \n    # Iteriamo a batch\n    for i in tqdm(range(0, total_msgs, BATCH_SIZE), desc=\"Processing\"):\n        batch_slice = msgs[i : i + BATCH_SIZE]\n        \n        # Prepariamo i testi validi per il batch\n        valid_texts = []\n        valid_indices = [] # Indici relativi al batch (0..31)\n        \n        for idx, doc in enumerate(batch_slice):\n            txt = doc.get(\"message\", \"\")\n            if txt and len(txt) > 4:\n                valid_texts.append(txt)\n                valid_indices.append(idx)\n        \n        if not valid_texts: continue\n        \n        # Classificazione\n        scores = classifica_batch(valid_texts, tokenizer, model, device)\n        \n        # Analisi Risultati\n        for local_idx, score in zip(valid_indices, scores):\n            if score >= SOGLIA_CYBER:\n                \n                # Indice assoluto nella lista 'msgs' completa\n                abs_idx = i + local_idx\n                doc = msgs[abs_idx]\n                \n                # --- ESTRAZIONE CONTESTO (IN MEMORIA) ---\n                # Precedenti: prendiamo i 3 messaggi prima (se esistono)\n                # Li inseriamo in ordine cronologico: [messaggio-3, messaggio-2, messaggio-1]\n                ctx_prev = []\n                for k in range(1, 4): # Cerca fino a 3 messaggi indietro\n                    if abs_idx - k >= 0:\n                        prev_msg = msgs[abs_idx - k].get(\"message\", \"\")\n                        # Se il messaggio esiste, lo aggiungiamo all'inizio della lista\n                        if prev_msg: \n                            ctx_prev.insert(0, prev_msg[:150] + \"...\") # Tagliamo a 150 caratteri per non intasare il CSV\n                \n                # Successivi: prendiamo 1 messaggio dopo (se esiste) per vedere eventuali reazioni immediate\n                ctx_next = []\n                if abs_idx + 1 < total_msgs:\n                    next_msg = msgs[abs_idx + 1].get(\"message\", \"\")\n                    if next_msg: ctx_next.append(next_msg[:100] + \"...\")\n\n                # --- COSTRUZIONE RECORD ORDINATO ---\n                risultati.append({\n                    # 1. IDENTIFICAZIONE GRUPPO (Le colonne \"Testata\")\n                    \"group_id\": meta[\"group_tele_id\"],\n                    \"chat_name\": meta[\"group_name\"],\n                    \"country\": meta[\"country\"],\n                    \"topic\": meta[\"topic\"],\n                    \n                    # 2. CONTESTO PRECEDENTE (Fondamentale per CTI)\n                    \"context_pre\": \" | \".join(ctx_prev) if ctx_prev else \"N/A\",\n\n# 3. DETTAGLI MESSAGGIO SOSPETTO\n                    \"msg_id\": doc.get(\"id\"),\n                    \"date\": doc.get(\"date\"),\n                    \"user_id\": get_user_id(doc),\n                    \"reply_to_msg_id\": doc.get(\"reply_to\", {}).get(\"reply_to_msg_id\") if doc.get(\"reply_to\") else None,\n                    \"text\": doc.get(\"message\"), # Il messaggio classificato come Cyber\n                    \n                    # 4. CONTESTO SUCCESSIVO E METADATI EXTRA\n                    \"context_next\": \" | \".join(ctx_next) if ctx_next else \"N/A\",\n                    \"language\": meta[\"language\"],\n                    \n                    # 5. SCORE DEL MODELLO\n                    \"cyber_score\": round(score, 4)\n                })\n    # Salvataggio\n    if risultati:\n        safe_name = re.sub(r'[\\\\/*?:\"<>|]', \"\", meta['group_name']).replace(\" \", \"_\")\n        df = pd.DataFrame(risultati)\n        df.to_csv(f\"{OUTPUT_FOLDER}/{safe_name}_CTI.csv\", index=False)\n        print(f\"   ‚úÖ Salvate {len(risultati)} minacce.\")\n    else:\n        print(\"   üí§ Nessuna minaccia rilevante.\")\n\nclient.close()\nprint(\"\\nüèÅ Pipeline CTI completata.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T15:21:45.595858Z","iopub.execute_input":"2025-12-08T15:21:45.596593Z","iopub.status.idle":"2025-12-08T16:53:44.607242Z","shell.execute_reply.started":"2025-12-08T15:21:45.596563Z","shell.execute_reply":"2025-12-08T16:53:44.606612Z"}},"outputs":[{"name":"stdout","text":"‚öôÔ∏è  Device: CUDA\nüîå Connessione DB...\nüìñ Costruzione Registro Gruppi...\n‚úÖ Mappati 12 gruppi target.\n‚úÖ Modello caricato.\n\nüìÇ Analisi: HTTP Injector Chat (Technologies)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 392/392 [01:27<00:00,  4.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 1264 minacce.\n\nüìÇ Analisi: Cyber Security - Information Security - IT Security - Experts (Technologies)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 149/149 [00:42<00:00,  3.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 298 minacce.\n\nüìÇ Analisi: Hacking Realm (Darknet)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 65/65 [00:31<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 288 minacce.\n\nüìÇ Analisi: HTTP Injector (Software & Applications)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 584/584 [03:00<00:00,  3.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 4062 minacce.\n\nüìÇ Analisi: Mikrotik-Training (Technologies)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231/231 [01:54<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 8831 minacce.\n\nüìÇ Analisi: Only Dark (Darknet)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2640/2640 [27:18<00:00,  1.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 155579 minacce.\n\nüìÇ Analisi: –•–∞–∫–µ—Ä—ã |–ß–∞—Ç| ùìóùì™ùì¨ùì¥ùìÆùìªùìº ùì¨ùì±ùì™ùìΩ (Darknet)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 959/959 [05:59<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 16344 minacce.\n\nüìÇ Analisi: –¢–µ–Ω–µ–≤–æ–π –î–∞—Ä–∫–Ω–µ—Ç –ß–∞—Ç (Darknet)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1172/1172 [11:15<00:00,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 70883 minacce.\n\nüìÇ Analisi: DDOS‰∫§ÊµÅÊîªÂáª (Darknet)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 661/661 [07:39<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 41765 minacce.\n\nüìÇ Analisi: –ß–∞—Ç –¥–æ–∫—Å–µ—Ä–æ–≤ (Darknet)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4944/4944 [27:56<00:00,  2.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 63242 minacce.\n\nüìÇ Analisi: HackDroids ‚Äî –ß–∞—Ç (Software & Applications)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:04<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 231 minacce.\n\nüìÇ Analisi: VirusCheck Chat (Software & Applications)\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:46<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"   ‚úÖ Salvate 4670 minacce.\n\nüèÅ Pipeline CTI completata.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport shutil\nfrom IPython.display import FileLink, display\n\n# ==============================================================================\n# CONFIGURAZIONE\n# ==============================================================================\nCARTELLA_INPUT = \"Results_CTI_Enriched\"  # La cartella dove lo script precedente ha salvato i file\nNOME_FILE_MERGED = \"CTI_DATASET.csv\"\nNOME_ZIP_SINGOLI = \"All_Groups_Individual_CSVs\"\n\n# ==============================================================================\n# 1. UNIONE DEI CSV (MERGING)\n# ==============================================================================\nprint(f\"üìÇ Cerco file CSV nella cartella: {CARTELLA_INPUT}...\")\n\n# Trova tutti i file .csv usando glob\nfiles = glob.glob(os.path.join(CARTELLA_INPUT, \"*.csv\"))\n\nif not files:\n    print(\"‚ùå ERRORE: Nessun file CSV trovato. Hai eseguito lo script di classificazione?\")\nelse:\n    print(f\"üìã Trovati {len(files)} file. Inizio unione...\")\n    \n    lista_dataframe = []\n    \n    # Ciclo su ogni file\n    for file in files:\n        try:\n            # Legge il singolo CSV\n            df_temp = pd.read_csv(file)\n            # Se il file non √® vuoto, lo aggiungiamo alla lista\n            if not df_temp.empty:\n                lista_dataframe.append(df_temp)\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Errore leggendo il file {file}: {e}\")\n\n    # Concatenazione\n    if lista_dataframe:\n        df_finale = pd.concat(lista_dataframe, ignore_index=True)\n        \n        # Salvataggio del Master File\n        df_finale.to_csv(NOME_FILE_MERGED, index=False)\n        \n        print(f\"\\nüìä STATISTICHE DATASET UNICO:\")\n        print(f\"   - Totale righe (minacce): {len(df_finale)}\")\n        print(f\"   - Colonne: {list(df_finale.columns)}\")\n        print(f\"   ‚úÖ File Master creato: {NOME_FILE_MERGED}\")\n    else:\n        print(\"‚ö†Ô∏è Nessun dato valido da unire.\")\n\n# ==============================================================================\n# 2. PREPARAZIONE DOWNLOAD (ZIP + LINK)\n# ==============================================================================\nprint(\"\\nüì¶ Preparazione pacchetti per il download...\")\n\n# A. Zippiamo la cartella con i 12 file separati (per comodit√†)\nshutil.make_archive(NOME_ZIP_SINGOLI, 'zip', CARTELLA_INPUT)\nprint(f\"   ‚úÖ Archivio ZIP creato: {NOME_ZIP_SINGOLI}.zip\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"‚¨áÔ∏è  LINK PER IL DOWNLOAD (Clicca qui sotto)\")\nprint(\"=\"*50)\n\nprint(f\"\\n1Ô∏è‚É£  SCARICA IL DATASET MERGED (UNICO FILE - {len(df_finale)} righe):\")\ndisplay(FileLink(NOME_FILE_MERGED))\n\nprint(f\"\\n2Ô∏è‚É£  SCARICA L'ARCHIVIO CON I 12 FILE SEPARATI (.zip):\")\ndisplay(FileLink(f\"{NOME_ZIP_SINGOLI}.zip\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T16:55:22.692977Z","iopub.execute_input":"2025-12-08T16:55:22.693271Z","iopub.status.idle":"2025-12-08T16:55:50.609359Z","shell.execute_reply.started":"2025-12-08T16:55:22.693247Z","shell.execute_reply":"2025-12-08T16:55:50.608766Z"}},"outputs":[{"name":"stdout","text":"üìÇ Cerco file CSV nella cartella: Results_CTI_Enriched...\nüìã Trovati 12 file. Inizio unione...\n\nüìä STATISTICHE DATASET UNICO:\n   - Totale righe (minacce): 367457\n   - Colonne: ['group_id', 'chat_name', 'country', 'topic', 'context_pre', 'msg_id', 'date', 'user_id', 'reply_to_msg_id', 'text', 'context_next', 'language', 'cyber_score']\n   ‚úÖ File Master creato: CTI_DATASET.csv\n\nüì¶ Preparazione pacchetti per il download...\n   ‚úÖ Archivio ZIP creato: All_Groups_Individual_CSVs.zip\n\n==================================================\n‚¨áÔ∏è  LINK PER IL DOWNLOAD (Clicca qui sotto)\n==================================================\n\n1Ô∏è‚É£  SCARICA IL DATASET MERGED (UNICO FILE - 367457 righe):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/CTI_DATASET.csv","text/html":"<a href='CTI_DATASET.csv' target='_blank'>CTI_DATASET.csv</a><br>"},"metadata":{}},{"name":"stdout","text":"\n2Ô∏è‚É£  SCARICA L'ARCHIVIO CON I 12 FILE SEPARATI (.zip):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/All_Groups_Individual_CSVs.zip","text/html":"<a href='All_Groups_Individual_CSVs.zip' target='_blank'>All_Groups_Individual_CSVs.zip</a><br>"},"metadata":{}}],"execution_count":8}]}