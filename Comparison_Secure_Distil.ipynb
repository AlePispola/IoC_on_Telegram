{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ce38de",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa3 in position 45: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m     30\u001b[39m secure = load_many(SECURE_PATH)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m distil = \u001b[43mload_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDISTIL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Normalizza tipi e colonne chiave\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df, name \u001b[38;5;129;01min\u001b[39;00m [(secure, \u001b[33m\"\u001b[39m\u001b[33msecure\u001b[39m\u001b[33m\"\u001b[39m), (distil, \u001b[33m\"\u001b[39m\u001b[33mdistil\u001b[39m\u001b[33m\"\u001b[39m)]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mload_many\u001b[39m\u001b[34m(path_pattern)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNessun file trovato per pattern: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m dfs = [\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[32m     27\u001b[39m df = pd.concat(dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:325\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xa3 in position 45: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# 0) CONFIG\n",
    "# =========================\n",
    "# Metti qui i tuoi file o folder (supporta anche wildcard)\n",
    "DISTIL_PATH = r\"DistilBERT/DistilDB/DistilDB/__MACOSX/DistilDB/1-CTI_DATASET.csv\"\n",
    "SECURE_PATH = r\"SECUREBERT/SECURE_DBS/1-CTI_DATASET.csv\"\n",
    "\n",
    "# Chiave per matchare lo stesso messaggio tra dataset\n",
    "# Consigliato: (group_id, msg_id) o (chat_name, msg_id)\n",
    "KEY_COLS = [\"group_id\", \"msg_id\"]\n",
    "\n",
    "# Soglia (se vuoi controllare / ricalcolare)\n",
    "THRESH = 0.60\n",
    "\n",
    "# =========================\n",
    "# 1) LOAD (singolo file o wildcard)\n",
    "# =========================\n",
    "def load_many(path_pattern: str) -> pd.DataFrame:\n",
    "    paths = sorted(Path().glob(path_pattern))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"Nessun file trovato per pattern: {path_pattern}\")\n",
    "    dfs = [pd.read_csv(p) for p in paths]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "secure = load_many(SECURE_PATH)\n",
    "distil = load_many(DISTIL_PATH)\n",
    "\n",
    "# Normalizza tipi e colonne chiave\n",
    "for df, name in [(secure, \"secure\"), (distil, \"distil\")]:\n",
    "    missing = [c for c in KEY_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Mancano colonne chiave {missing} in dataset {name}\")\n",
    "    # msg_id spesso è int, ma per sicurezza lo forziamo a stringa uniforme\n",
    "    df[\"msg_id\"] = df[\"msg_id\"].astype(str)\n",
    "    df[\"group_id\"] = df[\"group_id\"].astype(str)\n",
    "\n",
    "# (Opzionale) se vuoi verificare che i CSV rispettino già la soglia:\n",
    "if \"cyber_score\" in secure.columns:\n",
    "    print(\"Secure: % sotto soglia (dovrebbe essere ~0 se hai già filtrato):\",\n",
    "          (secure[\"cyber_score\"] < THRESH).mean())\n",
    "if \"cyber_score\" in distil.columns:\n",
    "    print(\"Distil: % sotto soglia (dovrebbe essere ~0 se hai già filtrato):\",\n",
    "          (distil[\"cyber_score\"] < THRESH).mean())\n",
    "\n",
    "# Dedup: nel caso in cui lo stesso msg sia comparso più volte\n",
    "secure = secure.drop_duplicates(subset=KEY_COLS).copy()\n",
    "distil = distil.drop_duplicates(subset=KEY_COLS).copy()\n",
    "\n",
    "# =========================\n",
    "# 2) SET METRICS (overlap)\n",
    "# =========================\n",
    "secure_keys = set(map(tuple, secure[KEY_COLS].to_numpy()))\n",
    "distil_keys = set(map(tuple, distil[KEY_COLS].to_numpy()))\n",
    "\n",
    "inter = secure_keys & distil_keys\n",
    "union = secure_keys | distil_keys\n",
    "\n",
    "nS = len(secure_keys)\n",
    "nD = len(distil_keys)\n",
    "nI = len(inter)\n",
    "nU = len(union)\n",
    "\n",
    "pct_D_in_S = (nI / nD) * 100 if nD else 0\n",
    "pct_S_in_D = (nI / nS) * 100 if nS else 0\n",
    "jaccard = (nI / nU) if nU else 0\n",
    "\n",
    "print(\"\\n=== OVERLAP METRICS ===\")\n",
    "print(f\"Secure retained: {nS:,}\")\n",
    "print(f\"Distil retained: {nD:,}\")\n",
    "print(f\"Intersection:    {nI:,}\")\n",
    "print(f\"Union:           {nU:,}\")\n",
    "print(f\"% Distil covered by Secure (|D∩S|/|D|): {pct_D_in_S:.2f}%\")\n",
    "print(f\"% Secure covered by Distil (|D∩S|/|S|): {pct_S_in_D:.2f}%\")\n",
    "print(f\"Jaccard (|D∩S|/|D∪S|): {jaccard:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 3) BREAKDOWN PER GROUP\n",
    "# =========================\n",
    "# Conteggio per gruppo\n",
    "Sg = secure.groupby(\"chat_name\", dropna=False).size().rename(\"secure_n\").reset_index()\n",
    "Dg = distil.groupby(\"chat_name\", dropna=False).size().rename(\"distil_n\").reset_index()\n",
    "\n",
    "# Intersection per gruppo: facciamo merge sulle chiavi e contiamo\n",
    "merged = secure[KEY_COLS + [\"chat_name\"]].merge(\n",
    "    distil[KEY_COLS + [\"chat_name\"]],\n",
    "    on=KEY_COLS,\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_S\", \"_D\"),\n",
    ")\n",
    "Ig = merged.groupby(\"chat_name_S\", dropna=False).size().rename(\"intersection_n\").reset_index()\n",
    "Ig = Ig.rename(columns={\"chat_name_S\": \"chat_name\"})\n",
    "\n",
    "by_group = Sg.merge(Dg, on=\"chat_name\", how=\"outer\").merge(Ig, on=\"chat_name\", how=\"left\")\n",
    "by_group = by_group.fillna(0)\n",
    "by_group[\"secure_n\"] = by_group[\"secure_n\"].astype(int)\n",
    "by_group[\"distil_n\"] = by_group[\"distil_n\"].astype(int)\n",
    "by_group[\"intersection_n\"] = by_group[\"intersection_n\"].astype(int)\n",
    "\n",
    "# percentuali per gruppo\n",
    "by_group[\"pct_D_in_S\"] = np.where(by_group[\"distil_n\"] > 0,\n",
    "                                  100 * by_group[\"intersection_n\"] / by_group[\"distil_n\"],\n",
    "                                  0)\n",
    "by_group[\"pct_S_in_D\"] = np.where(by_group[\"secure_n\"] > 0,\n",
    "                                  100 * by_group[\"intersection_n\"] / by_group[\"secure_n\"],\n",
    "                                  0)\n",
    "\n",
    "# ordina per divergenza: (secure - distil) o solo secure\n",
    "by_group[\"delta_S_minus_D\"] = by_group[\"secure_n\"] - by_group[\"distil_n\"]\n",
    "\n",
    "print(\"\\n=== TOP GROUPS BY (Secure - Distil) ===\")\n",
    "print(by_group.sort_values(\"delta_S_minus_D\", ascending=False)\n",
    "      .head(10)[[\"chat_name\", \"secure_n\", \"distil_n\", \"intersection_n\", \"delta_S_minus_D\", \"pct_D_in_S\", \"pct_S_in_D\"]])\n",
    "\n",
    "# =========================\n",
    "# 4) DISCORDANT SETS: only-secure / only-distil\n",
    "# =========================\n",
    "only_secure_keys = secure_keys - distil_keys\n",
    "only_distil_keys = distil_keys - secure_keys\n",
    "\n",
    "only_secure = secure.merge(\n",
    "    pd.DataFrame(list(only_secure_keys), columns=KEY_COLS),\n",
    "    on=KEY_COLS,\n",
    "    how=\"inner\"\n",
    ")\n",
    "only_distil = distil.merge(\n",
    "    pd.DataFrame(list(only_distil_keys), columns=KEY_COLS),\n",
    "    on=KEY_COLS,\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== DISCORDANT COUNTS ===\")\n",
    "print(f\"Only Secure: {len(only_secure):,}\")\n",
    "print(f\"Only Distil: {len(only_distil):,}\")\n",
    "\n",
    "# =========================\n",
    "# 5) SIMPLE TEXT STATS + PATTERN COUNTS (discordant analysis)\n",
    "# =========================\n",
    "def add_text_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"text_len\"] = df[\"text\"].astype(str).str.len()\n",
    "    df[\"word_len\"] = df[\"text\"].astype(str).str.split().apply(len)\n",
    "\n",
    "    # pattern \"grezzi\" (non IoC extraction completa, solo indicatori veloci)\n",
    "    df[\"has_url\"] = df[\"text\"].astype(str).str.contains(r\"(https?://|www\\.)\", regex=True)\n",
    "    df[\"has_cve\"] = df[\"text\"].astype(str).str.contains(r\"\\bCVE-\\d{4}-\\d+\\b\", regex=True, case=False)\n",
    "    df[\"has_ip\"]  = df[\"text\"].astype(str).str.contains(r\"\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b\", regex=True)\n",
    "    df[\"has_domain_like\"] = df[\"text\"].astype(str).str.contains(\n",
    "        r\"\\b(?:[a-zA-Z0-9-]+\\.)+(?:com|org|net|io|ru|cn|it|uk|gov)\\b\",\n",
    "        regex=True\n",
    "    )\n",
    "    return df\n",
    "\n",
    "only_secure = add_text_stats(only_secure)\n",
    "only_distil = add_text_stats(only_distil)\n",
    "\n",
    "def summarize(df: pd.DataFrame, name: str):\n",
    "    if df.empty:\n",
    "        print(f\"\\n[{name}] dataset vuoto.\")\n",
    "        return\n",
    "    print(f\"\\n=== {name} SUMMARY ===\")\n",
    "    print(f\"N: {len(df):,}\")\n",
    "    print(\"text_len (mean/median):\", df[\"text_len\"].mean().round(2), \"/\", df[\"text_len\"].median())\n",
    "    print(\"word_len (mean/median):\", df[\"word_len\"].mean().round(2), \"/\", df[\"word_len\"].median())\n",
    "    for col in [\"has_url\", \"has_cve\", \"has_ip\", \"has_domain_like\"]:\n",
    "        print(f\"{col}: {(df[col].mean()*100):.2f}%\")\n",
    "\n",
    "summarize(only_secure, \"ONLY SECURE\")\n",
    "summarize(only_distil, \"ONLY DISTIL\")\n",
    "\n",
    "# =========================\n",
    "# 6) OPTIONAL: SAVE REPORT CSVs\n",
    "# =========================\n",
    "out_dir = Path(\"compare_reports\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "by_group.sort_values(\"delta_S_minus_D\", ascending=False).to_csv(out_dir / \"by_group_overlap.csv\", index=False)\n",
    "only_secure.to_csv(out_dir / \"only_secure.csv\", index=False)\n",
    "only_distil.to_csv(out_dir / \"only_distil.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSaved reports to: {out_dir.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
